# From CUDA to Ascend in 10 Minutes

# ä» CUDA è¿ç§»åˆ°æ˜‡è…¾ï¼š10 åˆ†é’ŸæŒ‡å—

---

## Overview / æ¦‚è¿°

This guide walks you through migrating a PyTorch training script from NVIDIA CUDA to Huawei Ascend NPU using ascend-compat.

æœ¬æŒ‡å—å¸®åŠ©æ‚¨ä½¿ç”¨ ascend-compat å°† PyTorch è®­ç»ƒè„šæœ¬ä» NVIDIA CUDA è¿ç§»åˆ°åä¸ºæ˜‡è…¾ NPUã€‚

---

## Step 1: Install (2 minutes) / ç¬¬ä¸€æ­¥ï¼šå®‰è£…

```bash
# Install ascend-compat
pip install ascend-compat

# On your Ascend machine, also install torch_npu:
# (Match your PyTorch version â€” see compatibility matrix)
pip install torch-npu==2.4.0  # for PyTorch 2.4.0

# Verify your environment:
ascend-compat doctor
```

Expected output:

```
ascend-compat doctor â€” environment check
==================================================
  [OK] Python: Python 3.10.12
  [OK] PyTorch: PyTorch 2.4.0
  [OK] torch_npu: torch_npu 2.4.0
  [OK] CANN: CANN 8.0.RC2
  [OK] Compatibility: torch_npu 2.4.0 + PyTorch 2.4.0 â€” compatible
  [OK] NPU Device: 8 NPU(s) available: Ascend 910B
==================================================
  All checks passed!
```

---

## Step 2: One-line migration (1 minute) / ç¬¬äºŒæ­¥ï¼šä¸€è¡Œä»£ç è¿ç§»

### Before / è¿ç§»å‰

```python
import torch
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().cuda()
x = torch.randn(32, 10, device="cuda")

torch.cuda.manual_seed(42)
torch.backends.cudnn.benchmark = True

# Training loop...
with torch.cuda.amp.autocast():
    output = model(x)
```

### After / è¿ç§»å

```python
import ascend_compat  # â† Add this ONE line
import torch
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().cuda()
x = torch.randn(32, 10, device="cuda")

torch.cuda.manual_seed(42)
torch.backends.cudnn.benchmark = True

# Training loop...
with torch.cuda.amp.autocast():
    output = model(x)
```

**That's it.**  The shim transparently handles:
- `torch.device("cuda")` â†’ `torch.device("npu")`
- `model.cuda()` â†’ `model.npu()`
- `torch.cuda.manual_seed(42)` â†’ `torch.npu.manual_seed(42)`
- `torch.backends.cudnn.benchmark = True` â†’ no-op (safe)

**å°±è¿™æ ·ã€‚** é€‚é…å±‚è‡ªåŠ¨å¤„ç†æ‰€æœ‰ CUDA è°ƒç”¨çš„è½¬æ¢ã€‚

---

## Step 3: Check compatibility (2 minutes) / ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥å…¼å®¹æ€§

```bash
ascend-compat check train.py
```

Output:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ascend-compat migration check: train.py                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Total CUDA references:  8                                  â•‘
â•‘  Migration difficulty:   easy                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  ğŸ”„ Needs wrapper (ascend-compat handles)                    â•‘
â•‘  L4: torch.cuda.is_available                                 â•‘
â•‘  L5: .cuda()                                                 â•‘
â•‘  L6: torch.cuda.manual_seed                                  â•‘
â•‘  ...                                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

If difficulty is **easy** or **moderate**: `import ascend_compat` handles everything.

If difficulty is **hard**: some operations need manual changes (see Step 5).

---

## Step 4: FlashAttention (2 minutes) / ç¬¬å››æ­¥ï¼šFlashAttention

The `flash-attn` package cannot install on Ascend.  ascend-compat provides a drop-in replacement:

flash-attn åŒ…æ— æ³•åœ¨æ˜‡è…¾ä¸Šå®‰è£…ã€‚ascend-compat æä¾›äº†æ›¿ä»£æ–¹æ¡ˆï¼š

```python
import ascend_compat
from ascend_compat.ecosystem import transformers_patch
transformers_patch.apply()

# Now flash_attention_2 works!
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-7B",
    attn_implementation="flash_attention_2",
    device_map="auto",
    torch_dtype=torch.float16,
)
```

Or use the launcher for zero-code-change:

```bash
# Run any script with full shims â€” no code changes needed:
ascend-compat run train.py --batch-size 32
```

---

## Step 5: Handle hard cases (3 minutes) / ç¬¬äº”æ­¥ï¼šå¤„ç†å›°éš¾æƒ…å†µ

### FP64 operations / FP64 è¿ç®—

Ascend does not support FP64.  If your code uses `torch.float64`:

æ˜‡è…¾ä¸æ”¯æŒ FP64ã€‚å¦‚æœä»£ç ä½¿ç”¨äº† `torch.float64`ï¼š

```python
from ascend_compat.cuda_shim.dtype_manager import apply_dtype_policy, DTypePolicy

# Automatically substitute float64 â†’ float32
apply_dtype_policy(DTypePolicy.WARN)
```

### CUDA Graphs / CUDA å›¾

CUDA Graphs don't exist on Ascend.  Use `torch.compile` with the torchair backend:

CUDA å›¾åœ¨æ˜‡è…¾ä¸Šä¸å­˜åœ¨ã€‚ä½¿ç”¨ `torch.compile` é…åˆ torchair åç«¯ï¼š

```python
# Before (CUDA):
# g = torch.cuda.CUDAGraph()
# with torch.cuda.graph(g):
#     output = model(x)

# After (Ascend):
model = torch.compile(model, backend="torchair")
output = model(x)
```

### Distributed Training / åˆ†å¸ƒå¼è®­ç»ƒ

Replace NCCL with HCCL:

å°† NCCL æ›¿æ¢ä¸º HCCLï¼š

```python
import ascend_compat
from ascend_compat.ecosystem import deepspeed_patch
deepspeed_patch.apply()

# DeepSpeed will now use HCCL automatically
import deepspeed
deepspeed.init_distributed()  # Uses HCCL, not NCCL
```

Or manually:

```python
import torch.distributed as dist
dist.init_process_group(backend="hccl")  # Instead of "nccl"
```

### CANN Error Codes / CANN é”™è¯¯ç 

When you hit a cryptic error:

é‡åˆ°éš¾ä»¥ç†è§£çš„é”™è¯¯æ—¶ï¼š

```bash
ascend-compat error 507035
# â†’ CANN 507035: Operator execution failed â€” internal kernel error.
#   Likely cause: Unsupported dtype (e.g. FP64) or tensor shape doesn't
#   meet alignment requirements
#   Fix: Check input dtypes â€” use FP16 or FP32...
```

---

## Step 6: Monitor performance (optional) / ç¬¬å…­æ­¥ï¼šç›‘æ§æ€§èƒ½

### Detect CPU fallback ops / æ£€æµ‹ CPU å›é€€

```python
from ascend_compat.doctor import FallbackMonitor

monitor = FallbackMonitor()
with monitor:
    for batch in dataloader:
        output = model(batch)

print(monitor.report.summary())
# â†’ "2 CPU fallback(s): aten::histc (47 calls), aten::_unique2 (3 calls)"
```

### Audit a model before deployment / éƒ¨ç½²å‰å®¡è®¡

```python
from ascend_compat.doctor import audit_model
import torch

model = MyModel()
sample = torch.randn(1, 3, 224, 224)
report = audit_model(model, sample)
print(report.summary())
# â†’ "Operator Coverage: 98.5% native, 2 CPU fallback ops"
```

---

## Common Issues / å¸¸è§é—®é¢˜

| Issue | Cause | Fix |
|-------|-------|-----|
| `Torch not compiled with CUDA enabled` | Missing `import ascend_compat` | Add it as first import |
| NCCL timeout in distributed training | Wrong backend selected | Use `deepspeed_patch.apply()` or `backend="hccl"` |
| FlashAttention import fails | `flash-attn` not on Ascend | Use `transformers_patch.apply()` |
| Very slow training | CPU fallback ops | Run `FallbackMonitor` to identify |
| `ERR99999 UNKNOWN` | Version mismatch | Run `ascend-compat doctor` |
| OOM despite enough memory | Memory fragmentation | `torch.npu.empty_cache()` + reduce batch size |

---

## Performance Tips / æ€§èƒ½å»ºè®®

1. **Use FP16/BF16** â€” The Cube Unit is 4-8x faster at FP16 than FP32
2. **Avoid dynamic shapes** â€” Static shapes enable CANN graph optimization
3. **Use torch.compile** â€” `torch.compile(model, backend="torchair")` enables kernel fusion
4. **Batch small ops** â€” Ascend prefers large matrix operations over many small ones
5. **Check fallback ops** â€” A single CPU fallback in the critical path can destroy throughput

---

## Environment Variables / ç¯å¢ƒå˜é‡

```bash
# See every API translation (debugging):
export ASCEND_COMPAT_LOG_LEVEL=DEBUG

# Disable auto-patching:
export ASCEND_COMPAT_NO_PATCH=1

# Control NPU visibility:
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3

# CANN debug logging:
export ASCEND_GLOBAL_LOG_LEVEL=1
```

---

## What's Next / ä¸‹ä¸€æ­¥

- Join the community: [GitHub Issues](https://github.com/ascend-compat/ascend-compat/issues)
- Report incompatible operators
- Contribute mappings for new torch_npu versions
- Star the repo if it saved you migration time!
