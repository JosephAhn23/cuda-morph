# CUDA ‚Üí Ascend Compatibility Matrix

This document maps `torch.cuda` APIs to their Ascend NPU equivalents as
handled by ascend-compat's mapping registry (`cuda_shim._registry`).

## Status Legend

| Icon | Status | Meaning |
|------|--------|---------|
| ‚úÖ | **Direct** | Identical semantics.  `torch.cuda.X` ‚Üí `torch.npu.X` with no changes. |
| üîÑ | **Adapted** | Same concept but needs argument/return-value transformation. |
| ‚ùå | **Unsupported** | No Ascend equivalent.  Raises `NotImplementedError` with guidance. |

---

## Device Management

| CUDA API | Status | Ascend Equivalent | Notes |
|----------|--------|-------------------|-------|
| `torch.cuda.is_available()` | ‚úÖ | `torch.npu.is_available()` | **Returns False on Ascend** (prevents NCCL misdetection) |
| `torch.cuda.device_count()` | ‚úÖ | `torch.npu.device_count()` | |
| `torch.cuda.current_device()` | ‚úÖ | `torch.npu.current_device()` | |
| `torch.cuda.set_device(n)` | ‚úÖ | `torch.npu.set_device(n)` | |
| `torch.cuda.get_device_name()` | ‚úÖ | `torch.npu.get_device_name()` | Returns "Ascend 910B" etc. |
| `torch.cuda.get_device_properties()` | ‚úÖ | `torch.npu.get_device_properties()` | No `compute_capability` field |
| `torch.cuda.synchronize()` | ‚úÖ | `torch.npu.synchronize()` | |
| `torch.device("cuda")` | ‚úÖ | `torch.device("npu")` | Patched at `torch.device` level |
| `Tensor.cuda()` | ‚úÖ | `Tensor.npu()` | Patched on `torch.Tensor` |
| `Module.cuda()` | ‚úÖ | `Module.npu()` | Patched on `torch.nn.Module` |

## Memory Management

| CUDA API | Status | Ascend Equivalent | Notes |
|----------|--------|-------------------|-------|
| `torch.cuda.memory_allocated()` | ‚úÖ | `torch.npu.memory_allocated()` | |
| `torch.cuda.max_memory_allocated()` | ‚úÖ | `torch.npu.max_memory_allocated()` | |
| `torch.cuda.memory_reserved()` | ‚úÖ | `torch.npu.memory_reserved()` | |
| `torch.cuda.max_memory_reserved()` | ‚úÖ | `torch.npu.max_memory_reserved()` | |
| `torch.cuda.empty_cache()` | ‚úÖ | `torch.npu.empty_cache()` | |
| `torch.cuda.reset_peak_memory_stats()` | ‚úÖ | `torch.npu.reset_peak_memory_stats()` | |
| `torch.cuda.memory_stats()` | ‚úÖ | `torch.npu.memory_stats()` | Key names may differ |
| `torch.cuda.memory_summary()` | ‚úÖ | `torch.npu.memory_summary()` | Output format differs |
| `torch.cuda.mem_get_info()` | ‚úÖ | `torch.npu.mem_get_info()` | torch_npu ‚â• 2.2.0 |
| `torch.cuda.set_per_process_memory_fraction()` | ‚úÖ | `torch.npu.set_per_process_memory_fraction()` | |
| `torch.cuda.memory_snapshot()` | ‚ùå | N/A | Use `ascend-compat doctor` |

## Streams & Events

| CUDA API | Status | Ascend Equivalent | Notes |
|----------|--------|-------------------|-------|
| `torch.cuda.Stream()` | ‚úÖ | `torch.npu.Stream()` | |
| `torch.cuda.Event()` | ‚úÖ | `torch.npu.Event()` | |
| `torch.cuda.current_stream()` | ‚úÖ | `torch.npu.current_stream()` | |
| `torch.cuda.default_stream()` | ‚úÖ | `torch.npu.default_stream()` | |
| `torch.cuda.set_stream()` | ‚úÖ | `torch.npu.set_stream()` | |

## Random Number Generation

| CUDA API | Status | Ascend Equivalent | Notes |
|----------|--------|-------------------|-------|
| `torch.cuda.manual_seed(n)` | ‚úÖ | `torch.npu.manual_seed(n)` | |
| `torch.cuda.manual_seed_all(n)` | ‚úÖ | `torch.npu.manual_seed_all(n)` | |
| `torch.cuda.seed()` | ‚úÖ | `torch.npu.seed()` | |
| `torch.cuda.initial_seed()` | ‚úÖ | `torch.npu.initial_seed()` | |
| `torch.cuda.get_rng_state()` | ‚úÖ | `torch.npu.get_rng_state()` | State not transferable across backends |
| `torch.cuda.set_rng_state()` | ‚úÖ | `torch.npu.set_rng_state()` | State not transferable across backends |

## CUDA Graphs & Profiling

| CUDA API | Status | Ascend Equivalent | Notes |
|----------|--------|-------------------|-------|
| `torch.cuda.CUDAGraph` | ‚ùå | N/A | Use `torch.compile` with torchair backend |
| `torch.cuda.graph` | ‚ùå | N/A | Use `torch.compile` with torchair |
| `torch.cuda.nvtx.*` | ‚ùå | N/A | Use Ascend `msprof` profiler |

## Ecosystem Compatibility

| Library | Issue | ascend-compat Fix |
|---------|-------|-------------------|
| **flash-attn** | Cannot install on Ascend | `ecosystem.flash_attn` wraps `npu_fusion_attention` |
| **HuggingFace Transformers** | `device_map="auto"` crashes | `ecosystem.transformers_patch` fixes device detection |
| **HuggingFace accelerate** | Selects NCCL instead of HCCL | `cuda_shim` returns `is_available()=False` |
| **DeepSpeed** | NCCL init fails; timer.py crashes | `ecosystem.deepspeed_patch` registers HCCL |

## Hardware Limitations (not solvable by software)

| Limitation | Details |
|------------|---------|
| No FP64 support | Ascend 910A Cube Unit only supports FP16 GEMM |
| No Triton backend | Use TorchAir/torchair instead of torch.compile+Triton |
| 16-aligned shapes | Matrix dims must be multiples of 16 for Cube Unit |
| NC1HWC0 format | Internal memory layout differs from NCHW |
| Single PrivateUse1 | Only one custom backend per process |
