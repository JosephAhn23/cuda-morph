"""Generate Ascend C operator project from an OpSpec.

A complete Ascend C operator requires ~5-7 files versus CUDA's single .cu.
This module generates all required files from a declarative OpSpec:

    project_root/
    ├── CMakeLists.txt           # Build configuration
    ├── op_host/
    │   ├── {name}_tiling.h      # Tiling parameters (host→device)
    │   └── {name}.cpp           # Host-side: op registration + tiling logic
    ├── op_kernel/
    │   └── {name}.cpp           # Device-side: kernel implementation
    └── README.md                # Generated documentation

The generated kernel follows Ascend's canonical 3-stage pipeline:

    CopyIn (DMA: HBM→Local) → Compute (Vector/Cube) → CopyOut (DMA: Local→HBM)

With double-buffering (ping-pong) to hide DMA latency.
"""

from __future__ import annotations

import os
from typing import Dict

from ascend_compat._logging import get_logger
from ascend_compat.kernel_helper.spec import OpSpec

logger = get_logger(__name__)


def scaffold(spec: OpSpec, output_dir: str) -> Dict[str, str]:
    """Generate all files for an Ascend C operator project.

    Args:
        spec: Declarative operator specification.
        output_dir: Directory to create the project in (will be created).

    Returns:
        Dict mapping relative file paths to their generated contents.
    """
    files: Dict[str, str] = {}

    name = spec.name
    name_lower = name.lower()
    name_upper = name.upper()

    # ── Tiling header ────────────────────────────────────────────────
    files[f"op_host/{name_lower}_tiling.h"] = _gen_tiling_header(spec)

    # ── Host implementation ──────────────────────────────────────────
    files[f"op_host/{name_lower}.cpp"] = _gen_host_impl(spec)

    # ── Kernel implementation ────────────────────────────────────────
    files[f"op_kernel/{name_lower}.cpp"] = _gen_kernel_impl(spec)

    # ── CMakeLists.txt ───────────────────────────────────────────────
    files["CMakeLists.txt"] = _gen_cmake(spec)

    # ── README ───────────────────────────────────────────────────────
    files["README.md"] = _gen_readme(spec)

    # ── Write files ──────────────────────────────────────────────────
    for relpath, content in files.items():
        fullpath = os.path.join(output_dir, relpath)
        os.makedirs(os.path.dirname(fullpath), exist_ok=True)
        with open(fullpath, "w", encoding="utf-8") as f:
            f.write(content)
        logger.debug("Generated: %s", fullpath)

    logger.info(
        "Scaffolded Ascend C operator '%s' → %s (%d files)",
        name, output_dir, len(files),
    )

    return files


def _gen_tiling_header(spec: OpSpec) -> str:
    """Generate the tiling parameter struct header."""
    name = spec.name
    guard = f"{name.upper()}_TILING_H"

    attrs_fields = ""
    for attr_name, attr_type in spec.attrs.items():
        attrs_fields += f"    {attr_type} {attr_name};\n"

    return f"""\
#ifndef {guard}
#define {guard}

#include "register/tilingdata_base.h"

namespace optiling {{

// Tiling parameters computed on host, consumed by device kernel.
// These control how data is partitioned across AI Cores and
// how tiles fit into on-chip Local Memory (Unified Buffer).
BEGIN_TILING_DATA_DEF({name}TilingData)
    // Total number of elements to process
    TILING_DATA_FIELD_DEF(uint32_t, totalLength);
    // Number of tiles per AI Core
    TILING_DATA_FIELD_DEF(uint32_t, tileNum);
    // Elements per tile (must respect {spec.alignment}-byte alignment)
    TILING_DATA_FIELD_DEF(uint32_t, tileLength);
    // Remaining elements in the last (partial) tile
    TILING_DATA_FIELD_DEF(uint32_t, lastTileLength);
{attrs_fields}END_TILING_DATA_DEF;

// Register this tiling struct with CANN's tiling framework
REGISTER_TILING_DATA_CLASS({name}, {name}TilingData)

}}  // namespace optiling

#endif  // {guard}
"""


def _gen_host_impl(spec: OpSpec) -> str:
    """Generate host-side operator registration and tiling logic."""
    name = spec.name
    name_lower = name.lower()

    # Input/output registration
    io_lines = []
    for i, (tname, dtype) in enumerate(spec.inputs):
        cann_dtype = _to_cann_dtype(dtype)
        io_lines.append(f'    .INPUT(({tname}), TensorType({{DT_{cann_dtype}}}))  // input {i}')
    for i, (tname, dtype) in enumerate(spec.outputs):
        cann_dtype = _to_cann_dtype(dtype)
        io_lines.append(f'    .OUTPUT(({tname}), TensorType({{DT_{cann_dtype}}}))  // output {i}')

    io_block = "\n".join(io_lines)

    return f"""\
// Host-side implementation for {name} operator.
//
// This file handles:
// 1. Operator prototype registration (REG_OP)
// 2. Tiling computation — how to partition data across AI Cores
//    and fit tiles into on-chip Local Memory (Unified Buffer)
//
// Generated by ascend_compat.kernel_helper

#include "{name_lower}_tiling.h"
#include "register/op_def_registry.h"
#include "tiling/platform/platform_ascendc.h"

using namespace ge;
using namespace optiling;

namespace {{

// ═══════════════════════════════════════════════════════════════════
// Operator registration — declare inputs, outputs, and constraints
// ═══════════════════════════════════════════════════════════════════
REG_OP({name})
{io_block}
    .OP_END_FACTORY_REG({name})

// ═══════════════════════════════════════════════════════════════════
// Tiling implementation
// ═══════════════════════════════════════════════════════════════════
// Tiling divides the work across AI Cores.  Each core processes a
// subset of tiles, with each tile sized to fit in Local Memory.
//
// Key constraints:
// - Tile size must be aligned to {spec.alignment} bytes
// - Total Local Memory per core is typically 2-32 MB (hardware-dependent)
// - Double buffering requires 2x the single-tile memory
static ge::graphStatus TilingFunc(gert::TilingContext* context) {{
    {name}TilingData tiling;

    // Get total element count from the first input tensor shape
    uint32_t totalLength = context->GetInputShape(0)->GetStorageShape().GetShapeSize();
    tiling.set_totalLength(totalLength);

    // Query available Local Memory on this platform
    auto ascendcPlatform = platform_ascendc::PlatformAscendC(context->GetPlatformInfo());
    uint64_t ubSize = 0;
    ascendcPlatform.GetCoreMemSize(platform_ascendc::CoreMemType::UB, ubSize);

    // Data type size in bytes
    uint32_t dtypeSize = sizeof({_to_cpp_type(spec.inputs[0][1])});

    // Calculate max elements per tile that fit in UB with double buffering
    // Formula: ubSize / (numBuffers * numTensors * dtypeSize * alignment_factor)
    uint32_t numTensors = {len(spec.inputs) + len(spec.outputs)};
    uint32_t maxTileLen = ubSize / (2 * numTensors * dtypeSize);

    // Align down to {spec.alignment}-byte boundary
    uint32_t alignElements = {spec.alignment} / dtypeSize;
    maxTileLen = (maxTileLen / alignElements) * alignElements;

    // Compute tile count and sizes
    uint32_t tileNum = (totalLength + maxTileLen - 1) / maxTileLen;
    uint32_t tileLength = (tileNum > 1) ? maxTileLen : totalLength;
    uint32_t lastTileLength = totalLength - (tileNum - 1) * tileLength;

    tiling.set_tileNum(tileNum);
    tiling.set_tileLength(tileLength);
    tiling.set_lastTileLength(lastTileLength);

    // Set tiling block dimension (number of AI Cores to use)
    context->SetBlockDim(1);  // Single-core for now; scale later

    tiling.SaveToBuffer(context->GetRawTilingData()->GetData(),
                        context->GetRawTilingData()->GetCapacity());
    context->GetRawTilingData()->SetDataSize(tiling.GetDataSize());

    return ge::GRAPH_SUCCESS;
}}

}}  // anonymous namespace

// Register tiling function with CANN framework
REGISTER_TILING({name}, TilingFunc);
"""


def _gen_kernel_impl(spec: OpSpec) -> str:
    """Generate device-side kernel implementation."""
    name = spec.name
    name_lower = name.lower()

    # Build parameter list
    params = []
    for tname, dtype in spec.inputs:
        params.append(f"GM_ADDR {tname}")
    for tname, dtype in spec.outputs:
        params.append(f"GM_ADDR {tname}")

    params_str = ", ".join(params)

    # Build Init body — set up Global Memory tensor objects
    init_lines = []
    for i, (tname, dtype) in enumerate(spec.inputs):
        init_lines.append(
            f"        {tname}Gm_.SetGlobalBuffer((__gm__ {_to_cpp_type(dtype)}*){tname}"
            f" + block_idx * tileLength_, tileLength_);"
        )
    for i, (tname, dtype) in enumerate(spec.outputs):
        init_lines.append(
            f"        {tname}Gm_.SetGlobalBuffer((__gm__ {_to_cpp_type(dtype)}*){tname}"
            f" + block_idx * tileLength_, tileLength_);"
        )
    init_block = "\n".join(init_lines)

    # Build member declarations
    member_lines = []
    for tname, dtype in spec.inputs:
        member_lines.append(
            f"    GlobalTensor<{_to_cpp_type(dtype)}> {tname}Gm_;"
        )
    for tname, dtype in spec.outputs:
        member_lines.append(
            f"    GlobalTensor<{_to_cpp_type(dtype)}> {tname}Gm_;"
        )
    members_block = "\n".join(member_lines)

    # Pattern-specific compute body
    if spec.pattern == "elementwise":
        compute_body = _gen_elementwise_compute(spec)
    elif spec.pattern == "reduction":
        compute_body = _gen_reduction_compute(spec)
    elif spec.pattern == "matmul":
        compute_body = _gen_matmul_compute(spec)
    else:
        compute_body = _gen_custom_compute(spec)

    return f"""\
// Device-side kernel for {name} operator.
//
// Architecture: Ascend Da Vinci AI Core
// Pipeline: CopyIn → Compute → CopyOut (with double-buffering)
//
// Each AI Core contains:
//   - Scalar unit:  control flow, address computation
//   - Vector unit:  128-wide SIMD element-wise ops
//   - Cube unit:    16×16×16 matrix multiply-accumulate (FP16)
//
// Data flows through a 3-stage pipeline:
//   Stage 1 (CopyIn):  DMA transfer from HBM (Global Memory) to
//                       on-chip Local Memory (Unified Buffer)
//   Stage 2 (Compute): Vector or Cube operations on Local Memory data
//   Stage 3 (CopyOut): DMA transfer results back to HBM
//
// Double-buffering (ping-pong) overlaps Stage 1 of tile N+1 with
// Stage 2 of tile N, hiding DMA latency and raising bandwidth
// utilization from ~50% to ~85%.
//
// Generated by ascend_compat.kernel_helper

#include "kernel_operator.h"
#include "op_host/{name_lower}_tiling.h"

using namespace AscendC;

class Kernel{name} {{
public:
    __aicore__ inline Kernel{name}() {{}}

    // ═══════════════════════════════════════════════════════════════
    // Init — set up Global Memory handles and allocate Local buffers
    // ═══════════════════════════════════════════════════════════════
    __aicore__ inline void Init({params_str},
                                 optiling::{name}TilingData& tiling) {{
        totalLength_ = tiling.totalLength;
        tileNum_ = tiling.tileNum;
        tileLength_ = tiling.tileLength;
        lastTileLength_ = tiling.lastTileLength;

        ASSERT(GetBlockNum() != 0 && "block_num must not be zero");
        uint32_t block_idx = GetBlockIdx();

{init_block}

        // Allocate double-buffer queues for pipeline stages
        // VECIN_QUEUE / VECOUT_QUEUE are Ascend's queue identifiers
        // for the Vector pipeline
        pipe_.InitBuffer(inQueue_, 2, tileLength_ * sizeof({_to_cpp_type(spec.inputs[0][1])}));
        pipe_.InitBuffer(outQueue_, 2, tileLength_ * sizeof({_to_cpp_type(spec.outputs[0][1])}));
    }}

    // ═══════════════════════════════════════════════════════════════
    // Process — orchestrate the 3-stage pipeline across all tiles
    // ═══════════════════════════════════════════════════════════════
    __aicore__ inline void Process() {{
        for (uint32_t i = 0; i < tileNum_; i++) {{
            CopyIn(i);
            Compute(i);
            CopyOut(i);
        }}
    }}

private:
    // ─── Stage 1: CopyIn (HBM → Local Memory via DMA) ───────────
    __aicore__ inline void CopyIn(uint32_t tileIdx) {{
        LocalTensor<{_to_cpp_type(spec.inputs[0][1])}> inLocal = inQueue_.AllocTensor<{_to_cpp_type(spec.inputs[0][1])}>();
        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;
        DataCopy(inLocal, {spec.inputs[0][0]}Gm_[tileIdx * tileLength_], len);
        inQueue_.EnQue(inLocal);
    }}

    // ─── Stage 2: Compute (Vector/Cube operations on Local data) ─
    __aicore__ inline void Compute(uint32_t tileIdx) {{
{compute_body}
    }}

    // ─── Stage 3: CopyOut (Local Memory → HBM via DMA) ──────────
    __aicore__ inline void CopyOut(uint32_t tileIdx) {{
        LocalTensor<{_to_cpp_type(spec.outputs[0][1])}> outLocal = outQueue_.DeQue<{_to_cpp_type(spec.outputs[0][1])}>();
        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;
        DataCopy({spec.outputs[0][0]}Gm_[tileIdx * tileLength_], outLocal, len);
        outQueue_.FreeTensor(outLocal);
    }}

private:
    // Pipeline management (double-buffer queues)
    TPipe pipe_;
    TQue<QuePosition::VECIN, 2> inQueue_;
    TQue<QuePosition::VECOUT, 2> outQueue_;

    // Global Memory tensor handles
{members_block}

    // Tiling parameters
    uint32_t totalLength_ = 0;
    uint32_t tileNum_ = 0;
    uint32_t tileLength_ = 0;
    uint32_t lastTileLength_ = 0;
}};

// ═══════════════════════════════════════════════════════════════════
// Kernel entry point — launched by CANN runtime on each AI Core
// ═══════════════════════════════════════════════════════════════════
extern "C" __global__ __aicore__ void {name_lower}(
    {params_str}) {{

    // Read tiling parameters from the tiling data buffer
    optiling::{name}TilingData tilingData;
    GET_TILING_DATA(tilingData);

    Kernel{name} op;
    op.Init({", ".join(t[0] for t in spec.all_tensors)}, tilingData);
    op.Process();
}}
"""


def _gen_elementwise_compute(spec: OpSpec) -> str:
    """Generate compute body for elementwise pattern."""
    in_type = _to_cpp_type(spec.inputs[0][1])
    out_type = _to_cpp_type(spec.outputs[0][1])
    return f"""\
        // Dequeue input tile from CopyIn stage
        LocalTensor<{in_type}> inLocal = inQueue_.DeQue<{in_type}>();
        // Allocate output tile for CopyOut stage
        LocalTensor<{out_type}> outLocal = outQueue_.AllocTensor<{out_type}>();

        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;

        // ════════════════════════════════════════════════════════════
        // TODO: Replace with your element-wise computation
        // Example: Add(outLocal, inLocal, inLocal, len);
        //          Mul(outLocal, inLocal, scalar, len);
        //          Exp(outLocal, inLocal, len);
        //
        // Available Vector operations (Level 2 API):
        //   Arithmetic: Add, Sub, Mul, Div, Reciprocal
        //   Math:       Exp, Log, Sqrt, Rsqrt, Abs
        //   Compare:    Max, Min
        //   Activation: Relu, Sigmoid, Tanh
        // ════════════════════════════════════════════════════════════
        DataCopy(outLocal, inLocal, len);  // placeholder: identity

        inQueue_.FreeTensor(inLocal);
        outQueue_.EnQue(outLocal);"""


def _gen_reduction_compute(spec: OpSpec) -> str:
    """Generate compute body for reduction pattern."""
    in_type = _to_cpp_type(spec.inputs[0][1])
    out_type = _to_cpp_type(spec.outputs[0][1])
    return f"""\
        LocalTensor<{in_type}> inLocal = inQueue_.DeQue<{in_type}>();
        LocalTensor<{out_type}> outLocal = outQueue_.AllocTensor<{out_type}>();

        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;

        // ════════════════════════════════════════════════════════════
        // TODO: Replace with your reduction computation
        // Example: ReduceSum(outLocal, inLocal, workLocal, len);
        //          ReduceMax(outLocal, inLocal, workLocal, len);
        //          ReduceMin(outLocal, inLocal, workLocal, len);
        //
        // NOTE: Reductions on Ascend require a workspace tensor.
        //       Allocate from pipe_: pipe_.InitBuffer(workQueue, ...);
        // ════════════════════════════════════════════════════════════
        DataCopy(outLocal, inLocal, len);  // placeholder

        inQueue_.FreeTensor(inLocal);
        outQueue_.EnQue(outLocal);"""


def _gen_matmul_compute(spec: OpSpec) -> str:
    """Generate compute body for matmul pattern (Cube unit)."""
    return """\
        // ════════════════════════════════════════════════════════════
        // Matrix Multiply — Cube Unit (16×16×16 MAC per cycle)
        //
        // The Cube unit performs C = A × B where:
        //   A: [M, K] — must be padded to 16-element boundaries
        //   B: [K, N] — must be padded to 16-element boundaries
        //   C: [M, N] — output
        //
        // Use the Matmul API:
        //   Matmul<AType, BType, CType, BiasType> mm;
        //   mm.Init(tilingData);
        //   mm.SetTensorA(aGm_);
        //   mm.SetTensorB(bGm_);
        //   mm.IterateAll(cGm_);
        //   mm.End();
        //
        // IMPORTANT: Cube unit on 910A only supports FP16 natively.
        // FP32 GEMM is decomposed into 3 FP16 passes (77% peak).
        // 910B+ adds native FP32 at lower throughput.
        // ════════════════════════════════════════════════════════════
        LocalTensor<half> inLocal = inQueue_.DeQue<half>();
        LocalTensor<half> outLocal = outQueue_.AllocTensor<half>();

        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;
        DataCopy(outLocal, inLocal, len);  // placeholder

        inQueue_.FreeTensor(inLocal);
        outQueue_.EnQue(outLocal);"""


def _gen_custom_compute(spec: OpSpec) -> str:
    """Generate compute body placeholder for custom pattern."""
    in_type = _to_cpp_type(spec.inputs[0][1])
    out_type = _to_cpp_type(spec.outputs[0][1])
    return f"""\
        LocalTensor<{in_type}> inLocal = inQueue_.DeQue<{in_type}>();
        LocalTensor<{out_type}> outLocal = outQueue_.AllocTensor<{out_type}>();

        uint32_t len = (tileIdx == tileNum_ - 1) ? lastTileLength_ : tileLength_;

        // ════════════════════════════════════════════════════════════
        // TODO: Implement your custom computation here.
        //
        // Available pipeline units:
        //   Scalar: Control flow, address arithmetic
        //   Vector: 128-wide SIMD (Add, Mul, Exp, ReLU, etc.)
        //   Cube:   16×16×16 matrix MAC (Matmul API)
        //
        // Data movement:
        //   DataCopy(dst, src, len)    — DMA between memory levels
        //   EnQue / DeQue              — pipeline synchronization
        //
        // Refer to CANN Ascend C API docs:
        //   https://www.hiascend.com/document
        // ════════════════════════════════════════════════════════════
        DataCopy(outLocal, inLocal, len);  // placeholder

        inQueue_.FreeTensor(inLocal);
        outQueue_.EnQue(outLocal);"""


def _gen_cmake(spec: OpSpec) -> str:
    """Generate CMakeLists.txt for building the operator."""
    name = spec.name
    name_lower = name.lower()
    return f"""\
# CMakeLists.txt for {name} Ascend C operator
# Generated by ascend_compat.kernel_helper
#
# Build:
#   mkdir build && cd build
#   cmake .. -DCMAKE_BUILD_TYPE=Release
#   make -j$(nproc)
#
# Requires: CANN toolkit (ASCEND_HOME_PATH must be set)

cmake_minimum_required(VERSION 3.16)
project({name}Op LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# ── CANN Toolkit ──────────────────────────────────────────────────
if(DEFINED ENV{{ASCEND_HOME_PATH}})
    set(ASCEND_HOME $ENV{{ASCEND_HOME_PATH}})
else()
    set(ASCEND_HOME "/usr/local/Ascend/ascend-toolkit/latest")
endif()

message(STATUS "CANN toolkit: ${{ASCEND_HOME}}")

# Include CANN headers
include_directories(
    ${{ASCEND_HOME}}/include
    ${{ASCEND_HOME}}/include/aclnn
    ${{CMAKE_CURRENT_SOURCE_DIR}}/op_host
)

# ── Host-side library ─────────────────────────────────────────────
add_library({name_lower}_host SHARED
    op_host/{name_lower}.cpp
)
target_link_directories({name_lower}_host PRIVATE
    ${{ASCEND_HOME}}/lib64
)

# ── Kernel compilation ────────────────────────────────────────────
# Ascend C kernels are compiled by the Bisheng compiler (ccec).
# This is typically invoked via CANN's operator build framework:
#
#   python3 ${{ASCEND_HOME}}/python/site-packages/bin/msopgen gen \\
#       -i op_host/{name_lower}.json \\
#       -c ai_core-ascend910 \\
#       -out ./build
#
# For integration with torch_npu custom ops, see:
#   https://www.hiascend.com/document/detail/en/CANNCommercial/80RC1/devguide/opdevg

message(STATUS "Operator '{name}' configured. Use msopgen or CANN build tools to compile kernel.")
"""


def _gen_readme(spec: OpSpec) -> str:
    """Generate README for the operator project."""
    name = spec.name
    inputs_desc = ", ".join(f"`{n}` ({d})" for n, d in spec.inputs)
    outputs_desc = ", ".join(f"`{n}` ({d})" for n, d in spec.outputs)

    unit = "Cube (matrix multiply)" if spec.uses_cube else "Vector (SIMD)"

    return f"""\
# {name} — Ascend C Custom Operator

{spec.description or f'Custom {spec.pattern} operator for Ascend NPU.'}

## Architecture

| Property | Value |
|----------|-------|
| Pattern | {spec.pattern} |
| Compute Unit | {unit} |
| Inputs | {inputs_desc} |
| Outputs | {outputs_desc} |
| Alignment | {spec.alignment} bytes |

## Pipeline

```
CopyIn (HBM → Local Memory)
  ↓
Compute ({unit})
  ↓
CopyOut (Local Memory → HBM)
```

Double-buffering (ping-pong) overlaps CopyIn of tile N+1 with Compute
of tile N, raising bandwidth utilization from ~50% to ~85%.

## Build

```bash
export ASCEND_HOME_PATH=/usr/local/Ascend/ascend-toolkit/latest
mkdir build && cd build
cmake ..
make -j$(nproc)
```

## Integration with PyTorch (torch_npu)

```python
import torch
import torch_npu

# Register custom op via torch.utils.cpp_extension or
# the CANN operator package mechanism
torch.ops.load_library("build/lib{name.lower()}_host.so")

# Use in model
result = torch.ops.custom.{name.lower()}(input_tensor)
```

## Files

| File | Purpose |
|------|---------|
| `op_host/{name.lower()}_tiling.h` | Tiling parameter struct (host → device) |
| `op_host/{name.lower()}.cpp` | Host: op registration + tiling logic |
| `op_kernel/{name.lower()}.cpp` | Device: kernel implementation |
| `CMakeLists.txt` | Build configuration |

## Generated by

[ascend-compat](https://github.com/your-org/ascend-compat) kernel helper.
"""


# ---------------------------------------------------------------------------
# Type mapping helpers
# ---------------------------------------------------------------------------

def _to_cpp_type(dtype: str) -> str:
    """Map Python dtype string to C++ type."""
    mapping = {
        "float16": "half",
        "fp16": "half",
        "float32": "float",
        "fp32": "float",
        "bfloat16": "bfloat16_t",
        "bf16": "bfloat16_t",
        "int8": "int8_t",
        "int16": "int16_t",
        "int32": "int32_t",
        "uint8": "uint8_t",
    }
    return mapping.get(dtype.lower(), "half")


def _to_cann_dtype(dtype: str) -> str:
    """Map Python dtype string to CANN DT_ constant name."""
    mapping = {
        "float16": "FLOAT16",
        "fp16": "FLOAT16",
        "float32": "FLOAT",
        "fp32": "FLOAT",
        "bfloat16": "BF16",
        "bf16": "BF16",
        "int8": "INT8",
        "int16": "INT16",
        "int32": "INT32",
        "uint8": "UINT8",
    }
    return mapping.get(dtype.lower(), "FLOAT16")
